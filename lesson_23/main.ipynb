{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:50:06.550826Z",
     "start_time": "2025-11-26T12:50:06.437057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import mediapipe as mp\n",
    "\n",
    "person_model = YOLO(\"yolov8n.pt\")        # детектор людей\n",
    "face_model = mp.solutions.face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "tracker = person_model.track\n",
    "\n",
    "cap = cv2.VideoCapture(\"vid.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    track_results = person_model.track(frame, classes=[0], persist=True)[0]   # class=0 = person\n",
    "\n",
    "    annotated = track_results.plot()\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_results = face_model.process(rgb_frame)\n",
    "\n",
    "    if face_results.detections:\n",
    "        for det in face_results.detections:\n",
    "            bbox = det.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = int((bbox.xmin + bbox.width) * w)\n",
    "            y2 = int((bbox.ymin + bbox.height) * h)\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"People + Faces + Tracking\", annotated)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "8e21f67462f55600",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe' has no attribute 'solutions'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmediapipe\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmp\u001B[39;00m\n\u001B[32m      5\u001B[39m person_model = YOLO(\u001B[33m\"\u001B[39m\u001B[33myolov8n.pt\u001B[39m\u001B[33m\"\u001B[39m)        \u001B[38;5;66;03m# детектор людей\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m face_model = \u001B[43mmp\u001B[49m\u001B[43m.\u001B[49m\u001B[43msolutions\u001B[49m.face_detection.FaceDetection(model_selection=\u001B[32m1\u001B[39m, min_detection_confidence=\u001B[32m0.5\u001B[39m)\n\u001B[32m      8\u001B[39m tracker = person_model.track\n\u001B[32m     10\u001B[39m cap = cv2.VideoCapture(\u001B[33m\"\u001B[39m\u001B[33mvid.mp4\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: module 'mediapipe' has no attribute 'solutions'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:51:16.856785Z",
     "start_time": "2025-11-26T12:50:12.845270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import dlib\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "\n",
    "person_model = YOLO(\"yolov8n.pt\")  # или yolov11n.pt\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "dlib_detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "\n",
    "def is_smiling(landmarks):\n",
    "    left_corner = landmarks[48]\n",
    "    right_corner = landmarks[54]\n",
    "    top_lip = landmarks[51]\n",
    "    bottom_lip = landmarks[57]\n",
    "\n",
    "    middle_point = ((left_corner + right_corner) / 2).astype(int)\n",
    "\n",
    "    mouth_width = np.linalg.norm(left_corner - right_corner)\n",
    "    mouth_height = np.linalg.norm(top_lip - bottom_lip)\n",
    "\n",
    "    mouth_center_y = (top_lip[1] + bottom_lip[1]) / 2\n",
    "    left_lift = mouth_center_y - left_corner[1]\n",
    "    right_lift = mouth_center_y - right_corner[1]\n",
    "\n",
    "    smile_metric = (left_lift + right_lift)/2 + (mouth_width / mouth_height)\n",
    "    return smile_metric > 7.0, middle_point\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"vid_3.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = person_model.track(frame, classes=[0], persist=True)[0]  # class=0 = person\n",
    "\n",
    "    annotated = results.plot()\n",
    "\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        track_id = int(box.id) if box.id is not None else -1\n",
    "\n",
    "        person_roi = frame[y1:y2, x1:x2]\n",
    "        gray_roi = cv2.cvtColor(person_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = dlib_detector(gray_roi)\n",
    "\n",
    "        for face in faces:\n",
    "            fx1 = x1 + face.left()\n",
    "            fy1 = y1 + face.top()\n",
    "            fx2 = x1 + face.right()\n",
    "            fy2 = y1 + face.bottom()\n",
    "\n",
    "            cv2.rectangle(annotated, (fx1, fy1), (fx2, fy2), (0, 255, 0), 2)\n",
    "\n",
    "            # landmarks\n",
    "            shape = predictor(gray_roi, face)\n",
    "            shape = face_utils.shape_to_np(shape) + np.array([x1, y1])\n",
    "\n",
    "            # рисуем все 68 точек\n",
    "            for (i, (px, py)) in enumerate(shape):\n",
    "                cv2.circle(annotated, (px, py), 2, (0, 255, 0), -1)\n",
    "\n",
    "            # анализ глаз\n",
    "            left_eye = shape[42:48]\n",
    "            right_eye = shape[36:42]\n",
    "            ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "            eyes_open = ear > 0.25\n",
    "\n",
    "            # анализ улыбки\n",
    "            smiling, mid = is_smiling(shape)\n",
    "            cv2.circle(annotated, tuple(mid), 3, (0, 0, 255), -1)\n",
    "\n",
    "            # вывод информации\n",
    "            cv2.putText(annotated, f\"ID {track_id}\", (x1, y1-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            cv2.putText(annotated, f\"Eyes: {eyes_open}\", (fx1, fy1-25),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "            cv2.putText(annotated, f\"Smile: {smiling}\", (fx1, fy1-5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
    "\n",
    "    # показать\n",
    "    cv2.imshow(\"People + Face Analysis + Tracking\", annotated)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "c05409ba894cca51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 12 persons, 74.6ms\n",
      "Speed: 7.1ms preprocess, 74.6ms inference, 15.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 83.0ms\n",
      "Speed: 2.4ms preprocess, 83.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 97.9ms\n",
      "Speed: 2.0ms preprocess, 97.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 61.0ms\n",
      "Speed: 1.6ms preprocess, 61.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 56.4ms\n",
      "Speed: 1.7ms preprocess, 56.4ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.1ms\n",
      "Speed: 2.1ms preprocess, 48.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.6ms\n",
      "Speed: 1.9ms preprocess, 51.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 44.5ms\n",
      "Speed: 1.4ms preprocess, 44.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 46.4ms\n",
      "Speed: 1.7ms preprocess, 46.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.4ms\n",
      "Speed: 2.1ms preprocess, 48.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.9ms\n",
      "Speed: 1.7ms preprocess, 43.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.3ms\n",
      "Speed: 2.2ms preprocess, 45.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 42.8ms\n",
      "Speed: 1.5ms preprocess, 42.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.7ms\n",
      "Speed: 2.6ms preprocess, 47.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 51.9ms\n",
      "Speed: 2.0ms preprocess, 51.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 42.7ms\n",
      "Speed: 1.4ms preprocess, 42.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 79.4ms\n",
      "Speed: 1.8ms preprocess, 79.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 44.4ms\n",
      "Speed: 1.4ms preprocess, 44.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.3ms\n",
      "Speed: 1.5ms preprocess, 48.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.3ms\n",
      "Speed: 1.6ms preprocess, 48.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.1ms\n",
      "Speed: 2.1ms preprocess, 46.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 43.6ms\n",
      "Speed: 2.0ms preprocess, 43.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 49.2ms\n",
      "Speed: 2.0ms preprocess, 49.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.7ms\n",
      "Speed: 1.5ms preprocess, 50.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.1ms\n",
      "Speed: 2.3ms preprocess, 45.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.3ms\n",
      "Speed: 1.6ms preprocess, 46.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.2ms\n",
      "Speed: 2.2ms preprocess, 47.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.4ms\n",
      "Speed: 1.7ms preprocess, 47.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.6ms\n",
      "Speed: 1.6ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 45.7ms\n",
      "Speed: 1.7ms preprocess, 45.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.7ms\n",
      "Speed: 1.4ms preprocess, 44.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 75.0ms\n",
      "Speed: 2.7ms preprocess, 75.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 65.9ms\n",
      "Speed: 1.7ms preprocess, 65.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.2ms\n",
      "Speed: 2.3ms preprocess, 53.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 62.6ms\n",
      "Speed: 2.6ms preprocess, 62.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 60.5ms\n",
      "Speed: 2.1ms preprocess, 60.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 148.3ms\n",
      "Speed: 3.7ms preprocess, 148.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 75.0ms\n",
      "Speed: 2.1ms preprocess, 75.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 60.9ms\n",
      "Speed: 4.2ms preprocess, 60.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 57.8ms\n",
      "Speed: 1.8ms preprocess, 57.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 56.5ms\n",
      "Speed: 1.6ms preprocess, 56.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.3ms\n",
      "Speed: 2.3ms preprocess, 51.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.8ms\n",
      "Speed: 2.0ms preprocess, 47.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.1ms\n",
      "Speed: 1.6ms preprocess, 44.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.4ms\n",
      "Speed: 1.9ms preprocess, 47.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.5ms\n",
      "Speed: 1.4ms preprocess, 46.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 50.8ms\n",
      "Speed: 2.1ms preprocess, 50.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 44.3ms\n",
      "Speed: 1.5ms preprocess, 44.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 44.0ms\n",
      "Speed: 1.4ms preprocess, 44.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 41.5ms\n",
      "Speed: 1.7ms preprocess, 41.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 42.9ms\n",
      "Speed: 1.3ms preprocess, 42.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.3ms\n",
      "Speed: 1.4ms preprocess, 43.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 56.3ms\n",
      "Speed: 2.4ms preprocess, 56.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.1ms\n",
      "Speed: 1.5ms preprocess, 46.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 46.8ms\n",
      "Speed: 2.3ms preprocess, 46.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.4ms\n",
      "Speed: 1.3ms preprocess, 43.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.1ms\n",
      "Speed: 1.4ms preprocess, 43.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.7ms\n",
      "Speed: 1.5ms preprocess, 45.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.0ms\n",
      "Speed: 2.2ms preprocess, 45.0ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 58.9ms\n",
      "Speed: 1.7ms preprocess, 58.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 56.7ms\n",
      "Speed: 1.7ms preprocess, 56.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 49.3ms\n",
      "Speed: 2.1ms preprocess, 49.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 66.9ms\n",
      "Speed: 1.6ms preprocess, 66.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 63.5ms\n",
      "Speed: 1.5ms preprocess, 63.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 71.4ms\n",
      "Speed: 1.6ms preprocess, 71.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 84.4ms\n",
      "Speed: 1.6ms preprocess, 84.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 82.5ms\n",
      "Speed: 2.7ms preprocess, 82.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 42.1ms\n",
      "Speed: 1.7ms preprocess, 42.1ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.0ms\n",
      "Speed: 1.6ms preprocess, 51.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.4ms\n",
      "Speed: 2.0ms preprocess, 47.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 142.3ms\n",
      "Speed: 14.6ms preprocess, 142.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 40.5ms\n",
      "Speed: 1.7ms preprocess, 40.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.9ms\n",
      "Speed: 1.7ms preprocess, 50.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.4ms\n",
      "Speed: 1.6ms preprocess, 53.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 53.5ms\n",
      "Speed: 1.8ms preprocess, 53.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.9ms\n",
      "Speed: 1.6ms preprocess, 48.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.0ms\n",
      "Speed: 1.4ms preprocess, 46.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 60.6ms\n",
      "Speed: 1.8ms preprocess, 60.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 61.4ms\n",
      "Speed: 1.8ms preprocess, 61.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 47.8ms\n",
      "Speed: 2.0ms preprocess, 47.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.4ms\n",
      "Speed: 1.7ms preprocess, 48.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 60.2ms\n",
      "Speed: 1.8ms preprocess, 60.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 53.2ms\n",
      "Speed: 1.9ms preprocess, 53.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.7ms\n",
      "Speed: 1.4ms preprocess, 51.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pp/nnh10rws3jl0kc34jz4381080000gn/T/ipykernel_93754/43939586.py:36: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  smile_metric = (left_lift + right_lift)/2 + (mouth_width / mouth_height)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 15 persons, 45.9ms\n",
      "Speed: 2.0ms preprocess, 45.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.8ms\n",
      "Speed: 1.8ms preprocess, 47.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.1ms\n",
      "Speed: 1.8ms preprocess, 47.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.5ms\n",
      "Speed: 1.8ms preprocess, 52.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 95.1ms\n",
      "Speed: 2.2ms preprocess, 95.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 46.4ms\n",
      "Speed: 2.2ms preprocess, 46.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 53.6ms\n",
      "Speed: 1.6ms preprocess, 53.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.1ms\n",
      "Speed: 1.9ms preprocess, 46.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.3ms\n",
      "Speed: 1.6ms preprocess, 45.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 75.2ms\n",
      "Speed: 2.6ms preprocess, 75.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 96.7ms\n",
      "Speed: 2.2ms preprocess, 96.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 74.9ms\n",
      "Speed: 4.6ms preprocess, 74.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 78.7ms\n",
      "Speed: 2.8ms preprocess, 78.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 79.2ms\n",
      "Speed: 2.2ms preprocess, 79.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 68.4ms\n",
      "Speed: 2.3ms preprocess, 68.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 76.3ms\n",
      "Speed: 2.9ms preprocess, 76.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 84.7ms\n",
      "Speed: 2.1ms preprocess, 84.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 85.5ms\n",
      "Speed: 1.8ms preprocess, 85.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 110.5ms\n",
      "Speed: 3.9ms preprocess, 110.5ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 93.5ms\n",
      "Speed: 2.8ms preprocess, 93.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 74.3ms\n",
      "Speed: 1.9ms preprocess, 74.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 91.0ms\n",
      "Speed: 2.9ms preprocess, 91.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 74.5ms\n",
      "Speed: 2.4ms preprocess, 74.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 72.8ms\n",
      "Speed: 2.1ms preprocess, 72.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 92.6ms\n",
      "Speed: 2.4ms preprocess, 92.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 105.8ms\n",
      "Speed: 2.6ms preprocess, 105.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 92.0ms\n",
      "Speed: 2.7ms preprocess, 92.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 124.0ms\n",
      "Speed: 3.0ms preprocess, 124.0ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 126.9ms\n",
      "Speed: 4.0ms preprocess, 126.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 57.1ms\n",
      "Speed: 4.9ms preprocess, 57.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 81.9ms\n",
      "Speed: 1.6ms preprocess, 81.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 50.8ms\n",
      "Speed: 1.7ms preprocess, 50.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 55.2ms\n",
      "Speed: 1.6ms preprocess, 55.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 55.6ms\n",
      "Speed: 1.7ms preprocess, 55.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 51.1ms\n",
      "Speed: 1.7ms preprocess, 51.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 67.4ms\n",
      "Speed: 1.8ms preprocess, 67.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 49.1ms\n",
      "Speed: 2.0ms preprocess, 49.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 56.0ms\n",
      "Speed: 1.7ms preprocess, 56.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 44.6ms\n",
      "Speed: 1.8ms preprocess, 44.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 45.1ms\n",
      "Speed: 1.5ms preprocess, 45.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 47.9ms\n",
      "Speed: 1.7ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 53.7ms\n",
      "Speed: 1.5ms preprocess, 53.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 45.7ms\n",
      "Speed: 1.6ms preprocess, 45.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 49.9ms\n",
      "Speed: 1.9ms preprocess, 49.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 48.6ms\n",
      "Speed: 1.7ms preprocess, 48.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.7ms\n",
      "Speed: 1.5ms preprocess, 48.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 54.7ms\n",
      "Speed: 2.0ms preprocess, 54.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 59.5ms\n",
      "Speed: 1.9ms preprocess, 59.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.4ms\n",
      "Speed: 1.6ms preprocess, 48.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.6ms\n",
      "Speed: 1.7ms preprocess, 51.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 69.1ms\n",
      "Speed: 1.6ms preprocess, 69.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.5ms\n",
      "Speed: 1.9ms preprocess, 53.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.7ms\n",
      "Speed: 1.4ms preprocess, 47.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 49.3ms\n",
      "Speed: 1.8ms preprocess, 49.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.6ms\n",
      "Speed: 3.2ms preprocess, 46.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.2ms\n",
      "Speed: 1.4ms preprocess, 52.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 48.5ms\n",
      "Speed: 1.7ms preprocess, 48.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.4ms\n",
      "Speed: 1.4ms preprocess, 52.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.2ms\n",
      "Speed: 1.9ms preprocess, 44.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 52.1ms\n",
      "Speed: 1.8ms preprocess, 52.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 43.4ms\n",
      "Speed: 1.6ms preprocess, 43.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 52.7ms\n",
      "Speed: 1.8ms preprocess, 52.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 52.8ms\n",
      "Speed: 2.4ms preprocess, 52.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 50.2ms\n",
      "Speed: 2.0ms preprocess, 50.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 56.2ms\n",
      "Speed: 1.8ms preprocess, 56.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.8ms\n",
      "Speed: 2.8ms preprocess, 48.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 79.5ms\n",
      "Speed: 1.8ms preprocess, 79.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 61.5ms\n",
      "Speed: 2.1ms preprocess, 61.5ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 53.5ms\n",
      "Speed: 1.8ms preprocess, 53.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 45.7ms\n",
      "Speed: 1.7ms preprocess, 45.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 47.8ms\n",
      "Speed: 1.5ms preprocess, 47.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 62.3ms\n",
      "Speed: 3.5ms preprocess, 62.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 73.8ms\n",
      "Speed: 1.7ms preprocess, 73.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 56.0ms\n",
      "Speed: 1.7ms preprocess, 56.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 54.2ms\n",
      "Speed: 1.6ms preprocess, 54.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 64.9ms\n",
      "Speed: 1.9ms preprocess, 64.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 54.7ms\n",
      "Speed: 1.3ms preprocess, 54.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.9ms\n",
      "Speed: 1.6ms preprocess, 47.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.4ms\n",
      "Speed: 1.6ms preprocess, 53.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 55.1ms\n",
      "Speed: 2.1ms preprocess, 55.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.1ms\n",
      "Speed: 1.6ms preprocess, 51.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.0ms\n",
      "Speed: 2.3ms preprocess, 53.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 65.9ms\n",
      "Speed: 1.9ms preprocess, 65.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.8ms\n",
      "Speed: 2.0ms preprocess, 51.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.8ms\n",
      "Speed: 1.3ms preprocess, 46.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 55.8ms\n",
      "Speed: 1.7ms preprocess, 55.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.9ms\n",
      "Speed: 2.1ms preprocess, 50.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 59.3ms\n",
      "Speed: 1.9ms preprocess, 59.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 54.9ms\n",
      "Speed: 1.6ms preprocess, 54.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 47.2ms\n",
      "Speed: 1.8ms preprocess, 47.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.7ms\n",
      "Speed: 2.2ms preprocess, 52.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.8ms\n",
      "Speed: 2.5ms preprocess, 52.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 60.4ms\n",
      "Speed: 1.9ms preprocess, 60.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.2ms\n",
      "Speed: 1.4ms preprocess, 47.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.4ms\n",
      "Speed: 1.8ms preprocess, 51.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.0ms\n",
      "Speed: 2.3ms preprocess, 48.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 58.9ms\n",
      "Speed: 2.4ms preprocess, 58.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 59.2ms\n",
      "Speed: 2.6ms preprocess, 59.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 64.1ms\n",
      "Speed: 1.7ms preprocess, 64.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 42.6ms\n",
      "Speed: 1.5ms preprocess, 42.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 65.5ms\n",
      "Speed: 2.0ms preprocess, 65.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.0ms\n",
      "Speed: 1.8ms preprocess, 44.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 49.8ms\n",
      "Speed: 1.8ms preprocess, 49.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 143.2ms\n",
      "Speed: 1.7ms preprocess, 143.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 138.2ms\n",
      "Speed: 15.4ms preprocess, 138.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 83.0ms\n",
      "Speed: 9.8ms preprocess, 83.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 38.7ms\n",
      "Speed: 2.2ms preprocess, 38.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 40.6ms\n",
      "Speed: 1.5ms preprocess, 40.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.1ms\n",
      "Speed: 1.9ms preprocess, 46.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 43.9ms\n",
      "Speed: 1.7ms preprocess, 43.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 39.7ms\n",
      "Speed: 1.7ms preprocess, 39.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 41.3ms\n",
      "Speed: 1.5ms preprocess, 41.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 37.2ms\n",
      "Speed: 1.4ms preprocess, 37.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 40.6ms\n",
      "Speed: 1.6ms preprocess, 40.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.1ms\n",
      "Speed: 2.1ms preprocess, 45.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 66.6ms\n",
      "Speed: 2.8ms preprocess, 66.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 43.3ms\n",
      "Speed: 1.7ms preprocess, 43.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.2ms\n",
      "Speed: 1.8ms preprocess, 46.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 67.2ms\n",
      "Speed: 1.3ms preprocess, 67.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.5ms\n",
      "Speed: 1.7ms preprocess, 47.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.3ms\n",
      "Speed: 1.6ms preprocess, 50.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 48.3ms\n",
      "Speed: 1.6ms preprocess, 48.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 51.0ms\n",
      "Speed: 1.9ms preprocess, 51.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 43.3ms\n",
      "Speed: 1.7ms preprocess, 43.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 62.4ms\n",
      "Speed: 1.7ms preprocess, 62.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 81.8ms\n",
      "Speed: 2.6ms preprocess, 81.8ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 80.7ms\n",
      "Speed: 2.0ms preprocess, 80.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 73.4ms\n",
      "Speed: 2.4ms preprocess, 73.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 131.1ms\n",
      "Speed: 3.2ms preprocess, 131.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 63.9ms\n",
      "Speed: 1.9ms preprocess, 63.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 54.7ms\n",
      "Speed: 1.9ms preprocess, 54.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.6ms\n",
      "Speed: 1.6ms preprocess, 48.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 57.4ms\n",
      "Speed: 1.5ms preprocess, 57.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 68.6ms\n",
      "Speed: 2.1ms preprocess, 68.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.3ms\n",
      "Speed: 1.8ms preprocess, 47.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 50.4ms\n",
      "Speed: 1.6ms preprocess, 50.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.7ms\n",
      "Speed: 1.7ms preprocess, 47.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 46.7ms\n",
      "Speed: 1.7ms preprocess, 46.7ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 40.9ms\n",
      "Speed: 1.8ms preprocess, 40.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.4ms\n",
      "Speed: 1.5ms preprocess, 45.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.3ms\n",
      "Speed: 1.6ms preprocess, 48.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.3ms\n",
      "Speed: 1.5ms preprocess, 48.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.9ms\n",
      "Speed: 1.7ms preprocess, 46.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 45.2ms\n",
      "Speed: 1.8ms preprocess, 45.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 47.9ms\n",
      "Speed: 1.8ms preprocess, 47.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 51.5ms\n",
      "Speed: 1.6ms preprocess, 51.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.0ms\n",
      "Speed: 1.8ms preprocess, 44.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 51.4ms\n",
      "Speed: 1.4ms preprocess, 51.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.7ms\n",
      "Speed: 1.5ms preprocess, 51.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 42.2ms\n",
      "Speed: 1.4ms preprocess, 42.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 44.3ms\n",
      "Speed: 1.4ms preprocess, 44.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.2ms\n",
      "Speed: 1.8ms preprocess, 51.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 41.6ms\n",
      "Speed: 2.0ms preprocess, 41.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.1ms\n",
      "Speed: 1.4ms preprocess, 48.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 40.6ms\n",
      "Speed: 1.8ms preprocess, 40.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 74.3ms\n",
      "Speed: 2.0ms preprocess, 74.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 54.0ms\n",
      "Speed: 1.7ms preprocess, 54.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.1ms\n",
      "Speed: 1.5ms preprocess, 48.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.6ms\n",
      "Speed: 1.9ms preprocess, 46.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.9ms\n",
      "Speed: 1.8ms preprocess, 46.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 59.3ms\n",
      "Speed: 1.2ms preprocess, 59.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.8ms\n",
      "Speed: 1.7ms preprocess, 44.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.2ms\n",
      "Speed: 1.7ms preprocess, 44.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.3ms\n",
      "Speed: 2.2ms preprocess, 51.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.2ms\n",
      "Speed: 1.4ms preprocess, 46.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.0ms\n",
      "Speed: 1.6ms preprocess, 46.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 66.8ms\n",
      "Speed: 2.7ms preprocess, 66.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 44.7ms\n",
      "Speed: 1.7ms preprocess, 44.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 51.4ms\n",
      "Speed: 1.7ms preprocess, 51.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 52.7ms\n",
      "Speed: 1.6ms preprocess, 52.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 72.0ms\n",
      "Speed: 3.9ms preprocess, 72.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 58.5ms\n",
      "Speed: 1.7ms preprocess, 58.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 51.2ms\n",
      "Speed: 1.6ms preprocess, 51.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 58.7ms\n",
      "Speed: 1.7ms preprocess, 58.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.4ms\n",
      "Speed: 1.9ms preprocess, 50.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.0ms\n",
      "Speed: 1.8ms preprocess, 45.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.6ms\n",
      "Speed: 1.8ms preprocess, 46.6ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.4ms\n",
      "Speed: 1.8ms preprocess, 47.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 51.4ms\n",
      "Speed: 1.7ms preprocess, 51.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 43.8ms\n",
      "Speed: 1.8ms preprocess, 43.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 61.7ms\n",
      "Speed: 1.7ms preprocess, 61.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 64.0ms\n",
      "Speed: 2.1ms preprocess, 64.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 60.0ms\n",
      "Speed: 1.8ms preprocess, 60.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 49.4ms\n",
      "Speed: 2.0ms preprocess, 49.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 49.7ms\n",
      "Speed: 1.9ms preprocess, 49.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 69.6ms\n",
      "Speed: 2.3ms preprocess, 69.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 46.1ms\n",
      "Speed: 1.5ms preprocess, 46.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 52.1ms\n",
      "Speed: 1.9ms preprocess, 52.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 114.9ms\n",
      "Speed: 2.0ms preprocess, 114.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 66.4ms\n",
      "Speed: 3.0ms preprocess, 66.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 55.0ms\n",
      "Speed: 1.6ms preprocess, 55.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 70.4ms\n",
      "Speed: 1.9ms preprocess, 70.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 41.3ms\n",
      "Speed: 1.6ms preprocess, 41.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 65.2ms\n",
      "Speed: 1.7ms preprocess, 65.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 51.0ms\n",
      "Speed: 2.0ms preprocess, 51.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 48.5ms\n",
      "Speed: 1.8ms preprocess, 48.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 46.6ms\n",
      "Speed: 1.5ms preprocess, 46.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 44.4ms\n",
      "Speed: 1.6ms preprocess, 44.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 48.5ms\n",
      "Speed: 1.7ms preprocess, 48.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 54.2ms\n",
      "Speed: 1.9ms preprocess, 54.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.7ms\n",
      "Speed: 1.5ms preprocess, 47.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 44.7ms\n",
      "Speed: 1.7ms preprocess, 44.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 42.3ms\n",
      "Speed: 1.9ms preprocess, 42.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 46.9ms\n",
      "Speed: 2.5ms preprocess, 46.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 50.4ms\n",
      "Speed: 2.0ms preprocess, 50.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 236.0ms\n",
      "Speed: 5.5ms preprocess, 236.0ms inference, 11.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 123.6ms\n",
      "Speed: 4.8ms preprocess, 123.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 78.6ms\n",
      "Speed: 9.8ms preprocess, 78.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 48.7ms\n",
      "Speed: 2.0ms preprocess, 48.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 41.7ms\n",
      "Speed: 2.1ms preprocess, 41.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 37.7ms\n",
      "Speed: 1.7ms preprocess, 37.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 53.5ms\n",
      "Speed: 1.8ms preprocess, 53.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 49.4ms\n",
      "Speed: 1.7ms preprocess, 49.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 47.7ms\n",
      "Speed: 1.8ms preprocess, 47.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 54.0ms\n",
      "Speed: 1.6ms preprocess, 54.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 47.0ms\n",
      "Speed: 2.0ms preprocess, 47.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 53.7ms\n",
      "Speed: 1.6ms preprocess, 53.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 51.3ms\n",
      "Speed: 1.7ms preprocess, 51.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 50.8ms\n",
      "Speed: 1.7ms preprocess, 50.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 42.7ms\n",
      "Speed: 1.7ms preprocess, 42.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 55.4ms\n",
      "Speed: 1.9ms preprocess, 55.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 55.2ms\n",
      "Speed: 1.9ms preprocess, 55.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 92.0ms\n",
      "Speed: 2.1ms preprocess, 92.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 62.1ms\n",
      "Speed: 2.7ms preprocess, 62.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 60.3ms\n",
      "Speed: 2.1ms preprocess, 60.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 58.3ms\n",
      "Speed: 2.2ms preprocess, 58.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.9ms\n",
      "Speed: 1.9ms preprocess, 46.9ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.0ms\n",
      "Speed: 2.1ms preprocess, 53.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.3ms\n",
      "Speed: 1.9ms preprocess, 48.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.9ms\n",
      "Speed: 1.6ms preprocess, 51.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.4ms\n",
      "Speed: 1.8ms preprocess, 46.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.4ms\n",
      "Speed: 1.5ms preprocess, 46.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.3ms\n",
      "Speed: 1.7ms preprocess, 45.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.8ms\n",
      "Speed: 1.9ms preprocess, 50.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 53.1ms\n",
      "Speed: 1.8ms preprocess, 53.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 49.4ms\n",
      "Speed: 1.7ms preprocess, 49.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.8ms\n",
      "Speed: 1.5ms preprocess, 45.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.2ms\n",
      "Speed: 1.7ms preprocess, 45.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 50.7ms\n",
      "Speed: 1.8ms preprocess, 50.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 56.5ms\n",
      "Speed: 1.7ms preprocess, 56.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.5ms\n",
      "Speed: 2.1ms preprocess, 47.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 58.6ms\n",
      "Speed: 1.6ms preprocess, 58.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 48.0ms\n",
      "Speed: 1.5ms preprocess, 48.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 48.2ms\n",
      "Speed: 1.8ms preprocess, 48.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 53.6ms\n",
      "Speed: 1.4ms preprocess, 53.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.6ms\n",
      "Speed: 1.9ms preprocess, 52.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 42.5ms\n",
      "Speed: 1.4ms preprocess, 42.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 48.8ms\n",
      "Speed: 1.4ms preprocess, 48.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 50.5ms\n",
      "Speed: 1.5ms preprocess, 50.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.0ms\n",
      "Speed: 1.6ms preprocess, 51.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 45.7ms\n",
      "Speed: 1.7ms preprocess, 45.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 45.1ms\n",
      "Speed: 1.8ms preprocess, 45.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 182.6ms\n",
      "Speed: 2.7ms preprocess, 182.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 119.1ms\n",
      "Speed: 2.3ms preprocess, 119.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 69.6ms\n",
      "Speed: 2.1ms preprocess, 69.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 68.2ms\n",
      "Speed: 1.8ms preprocess, 68.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 64.2ms\n",
      "Speed: 2.2ms preprocess, 64.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 60.9ms\n",
      "Speed: 2.3ms preprocess, 60.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 51.0ms\n",
      "Speed: 1.7ms preprocess, 51.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 44.9ms\n",
      "Speed: 1.8ms preprocess, 44.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.1ms\n",
      "Speed: 2.3ms preprocess, 47.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.5ms\n",
      "Speed: 1.7ms preprocess, 46.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.2ms\n",
      "Speed: 1.8ms preprocess, 44.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 56.9ms\n",
      "Speed: 1.9ms preprocess, 56.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 52.1ms\n",
      "Speed: 2.2ms preprocess, 52.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 54.1ms\n",
      "Speed: 1.5ms preprocess, 54.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 74.8ms\n",
      "Speed: 1.7ms preprocess, 74.8ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 48.5ms\n",
      "Speed: 1.4ms preprocess, 48.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 43.0ms\n",
      "Speed: 1.7ms preprocess, 43.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 50.5ms\n",
      "Speed: 2.2ms preprocess, 50.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 53.0ms\n",
      "Speed: 1.7ms preprocess, 53.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 46.2ms\n",
      "Speed: 1.3ms preprocess, 46.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 61.6ms\n",
      "Speed: 1.6ms preprocess, 61.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.0ms\n",
      "Speed: 1.6ms preprocess, 50.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 47.9ms\n",
      "Speed: 1.6ms preprocess, 47.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 60.9ms\n",
      "Speed: 3.3ms preprocess, 60.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.5ms\n",
      "Speed: 1.4ms preprocess, 47.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 47.6ms\n",
      "Speed: 1.6ms preprocess, 47.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 52.0ms\n",
      "Speed: 1.8ms preprocess, 52.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 45.8ms\n",
      "Speed: 1.6ms preprocess, 45.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 44.7ms\n",
      "Speed: 1.7ms preprocess, 44.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 50.0ms\n",
      "Speed: 1.7ms preprocess, 50.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 50.7ms\n",
      "Speed: 1.4ms preprocess, 50.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 53.3ms\n",
      "Speed: 2.2ms preprocess, 53.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 44.4ms\n",
      "Speed: 1.6ms preprocess, 44.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 46.9ms\n",
      "Speed: 1.7ms preprocess, 46.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 40.4ms\n",
      "Speed: 1.8ms preprocess, 40.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 50.2ms\n",
      "Speed: 1.4ms preprocess, 50.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 46.9ms\n",
      "Speed: 1.7ms preprocess, 46.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 49.1ms\n",
      "Speed: 1.5ms preprocess, 49.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 53.3ms\n",
      "Speed: 1.8ms preprocess, 53.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 97\u001B[39m\n\u001B[32m     94\u001B[39m     \u001B[38;5;66;03m# показать\u001B[39;00m\n\u001B[32m     95\u001B[39m     cv2.imshow(\u001B[33m\"\u001B[39m\u001B[33mPeople + Face Analysis + Tracking\u001B[39m\u001B[33m\"\u001B[39m, annotated)\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcv2\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m & \u001B[32m0xFF\u001B[39m == \u001B[32m27\u001B[39m:\n\u001B[32m     98\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    100\u001B[39m cap.release()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:52:43.885372Z",
     "start_time": "2025-11-26T12:52:15.407294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from ultralytics import YOLO\n",
    "\n",
    "person_model = YOLO(\"yolov8n.pt\")\n",
    "tracker = None\n",
    "\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def is_smiling(landmarks):\n",
    "    left_corner = landmarks[48]\n",
    "    right_corner = landmarks[54]\n",
    "    top_lip = landmarks[51]\n",
    "    bottom_lip = landmarks[57]\n",
    "\n",
    "    mouth_width = np.linalg.norm(left_corner - right_corner)\n",
    "    mouth_height = np.linalg.norm(top_lip - bottom_lip)\n",
    "    mouth_center_y = (top_lip[1] + bottom_lip[1]) / 2\n",
    "    left_lift = mouth_center_y - left_corner[1]\n",
    "    right_lift = mouth_center_y - right_corner[1]\n",
    "\n",
    "    smile_metric = (left_lift + right_lift)/2 + (mouth_width / mouth_height)\n",
    "    is_smile = smile_metric > 7.0\n",
    "    middle_point = ((left_corner + right_corner) / 2).astype(int)\n",
    "    return is_smile, middle_point\n",
    "\n",
    "cap = cv2.VideoCapture(\"vid_3.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = person_model.track(frame, classes=[0], persist=True)[0]\n",
    "\n",
    "    annotated_frame = frame.copy()\n",
    "    for box, score, id in zip(results.boxes.xyxy, results.boxes.conf, results.boxes.id):\n",
    "        x1, y1, x2, y2 = [int(c) for c in box]\n",
    "        # Тонкая рамка\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(annotated_frame, f\"ID:{id}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # Рисуем 68 точек лица\n",
    "        for (i, (x, y)) in enumerate(shape):\n",
    "            cv2.circle(annotated_frame, (x, y), 1, (0, 255, 255), -1)\n",
    "\n",
    "        hull = cv2.convexHull(shape)\n",
    "        cv2.drawContours(annotated_frame, [hull], -1, (255, 0, 0), 1)\n",
    "\n",
    "        # Проверка глаз\n",
    "        left_eye = shape[42:48]\n",
    "        right_eye = shape[36:42]\n",
    "        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "\n",
    "        # Проверка улыбки\n",
    "        smiling, mouth_middle = is_smiling(shape)\n",
    "        eye_status = \"Open\" if ear > 0.25 else \"Closed\"\n",
    "        smile_status = \"Yes\" if smiling else \"No\"\n",
    "\n",
    "        cv2.circle(annotated_frame, tuple(mouth_middle), 2, (0, 0, 255), -1)\n",
    "        # Координаты лица\n",
    "        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "        # Рисуем текст чуть выше головы\n",
    "        cv2.putText(annotated_frame, f\"Eyes:{eye_status}\", (x1, y1 - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "        cv2.putText(annotated_frame, f\"Smile:{smile_status}\", (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "\n",
    "    cv2.imshow(\"People + Faces + Tracking\", annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # ESC для выхода\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "63b4a7b2be00c465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 12 persons, 76.4ms\n",
      "Speed: 2.5ms preprocess, 76.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 54.0ms\n",
      "Speed: 1.7ms preprocess, 54.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 60.5ms\n",
      "Speed: 2.8ms preprocess, 60.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 59.2ms\n",
      "Speed: 1.3ms preprocess, 59.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 54.5ms\n",
      "Speed: 1.8ms preprocess, 54.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.4ms\n",
      "Speed: 1.5ms preprocess, 50.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 49.9ms\n",
      "Speed: 1.6ms preprocess, 49.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.7ms\n",
      "Speed: 1.4ms preprocess, 48.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 45.6ms\n",
      "Speed: 1.3ms preprocess, 45.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 59.7ms\n",
      "Speed: 1.6ms preprocess, 59.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.3ms\n",
      "Speed: 2.0ms preprocess, 50.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.1ms\n",
      "Speed: 1.5ms preprocess, 48.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.6ms\n",
      "Speed: 1.5ms preprocess, 45.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 57.0ms\n",
      "Speed: 1.6ms preprocess, 57.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.9ms\n",
      "Speed: 1.4ms preprocess, 50.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 53.7ms\n",
      "Speed: 1.4ms preprocess, 53.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 50.1ms\n",
      "Speed: 2.1ms preprocess, 50.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.8ms\n",
      "Speed: 1.5ms preprocess, 46.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.3ms\n",
      "Speed: 1.8ms preprocess, 46.3ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 62.1ms\n",
      "Speed: 1.9ms preprocess, 62.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 54.0ms\n",
      "Speed: 1.9ms preprocess, 54.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 46.0ms\n",
      "Speed: 1.6ms preprocess, 46.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.1ms\n",
      "Speed: 1.4ms preprocess, 47.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 54.3ms\n",
      "Speed: 2.3ms preprocess, 54.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 40.8ms\n",
      "Speed: 1.4ms preprocess, 40.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 47.1ms\n",
      "Speed: 1.7ms preprocess, 47.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.7ms\n",
      "Speed: 2.1ms preprocess, 47.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.5ms\n",
      "Speed: 1.7ms preprocess, 46.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 59.9ms\n",
      "Speed: 1.7ms preprocess, 59.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.6ms\n",
      "Speed: 1.7ms preprocess, 47.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 68.4ms\n",
      "Speed: 1.4ms preprocess, 68.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 54.6ms\n",
      "Speed: 1.5ms preprocess, 54.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 55.8ms\n",
      "Speed: 1.9ms preprocess, 55.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.8ms\n",
      "Speed: 1.6ms preprocess, 52.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.8ms\n",
      "Speed: 1.4ms preprocess, 47.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.1ms\n",
      "Speed: 1.6ms preprocess, 46.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.0ms\n",
      "Speed: 1.3ms preprocess, 52.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 43.8ms\n",
      "Speed: 1.4ms preprocess, 43.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 49.8ms\n",
      "Speed: 1.9ms preprocess, 49.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.1ms\n",
      "Speed: 1.6ms preprocess, 52.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 49.5ms\n",
      "Speed: 2.0ms preprocess, 49.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 42.5ms\n",
      "Speed: 2.0ms preprocess, 42.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 48.7ms\n",
      "Speed: 2.2ms preprocess, 48.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 47.4ms\n",
      "Speed: 1.6ms preprocess, 47.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.9ms\n",
      "Speed: 1.8ms preprocess, 51.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 45.7ms\n",
      "Speed: 1.4ms preprocess, 45.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 58.7ms\n",
      "Speed: 1.4ms preprocess, 58.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 48.1ms\n",
      "Speed: 1.4ms preprocess, 48.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 57.5ms\n",
      "Speed: 4.1ms preprocess, 57.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 54.8ms\n",
      "Speed: 1.5ms preprocess, 54.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 47.0ms\n",
      "Speed: 1.9ms preprocess, 47.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 46.2ms\n",
      "Speed: 1.3ms preprocess, 46.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 62.8ms\n",
      "Speed: 1.5ms preprocess, 62.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.9ms\n",
      "Speed: 2.3ms preprocess, 52.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 58.7ms\n",
      "Speed: 1.6ms preprocess, 58.7ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.8ms\n",
      "Speed: 1.3ms preprocess, 48.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.9ms\n",
      "Speed: 1.8ms preprocess, 48.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.4ms\n",
      "Speed: 1.7ms preprocess, 50.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.6ms\n",
      "Speed: 2.0ms preprocess, 47.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 60.8ms\n",
      "Speed: 2.1ms preprocess, 60.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.8ms\n",
      "Speed: 1.6ms preprocess, 47.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.4ms\n",
      "Speed: 1.6ms preprocess, 52.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 50.0ms\n",
      "Speed: 1.7ms preprocess, 50.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 47.5ms\n",
      "Speed: 1.7ms preprocess, 47.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 45.0ms\n",
      "Speed: 1.6ms preprocess, 45.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.7ms\n",
      "Speed: 3.5ms preprocess, 44.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 41.9ms\n",
      "Speed: 1.7ms preprocess, 41.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 49.1ms\n",
      "Speed: 1.4ms preprocess, 49.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 43.5ms\n",
      "Speed: 2.3ms preprocess, 43.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 43.6ms\n",
      "Speed: 1.3ms preprocess, 43.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 44.5ms\n",
      "Speed: 1.5ms preprocess, 44.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 49.6ms\n",
      "Speed: 1.7ms preprocess, 49.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 48.1ms\n",
      "Speed: 1.7ms preprocess, 48.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 40.7ms\n",
      "Speed: 1.5ms preprocess, 40.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 46.7ms\n",
      "Speed: 1.4ms preprocess, 46.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 57.1ms\n",
      "Speed: 1.5ms preprocess, 57.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.1ms\n",
      "Speed: 1.5ms preprocess, 52.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 43.8ms\n",
      "Speed: 1.4ms preprocess, 43.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 49.3ms\n",
      "Speed: 1.4ms preprocess, 49.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 45.6ms\n",
      "Speed: 2.4ms preprocess, 45.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 41.9ms\n",
      "Speed: 1.8ms preprocess, 41.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 45.3ms\n",
      "Speed: 1.4ms preprocess, 45.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 46.8ms\n",
      "Speed: 1.8ms preprocess, 46.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 52.4ms\n",
      "Speed: 2.0ms preprocess, 52.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 70.7ms\n",
      "Speed: 1.9ms preprocess, 70.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 59.8ms\n",
      "Speed: 2.0ms preprocess, 59.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 55.5ms\n",
      "Speed: 1.7ms preprocess, 55.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 57.1ms\n",
      "Speed: 1.5ms preprocess, 57.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.8ms\n",
      "Speed: 1.7ms preprocess, 46.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 48.5ms\n",
      "Speed: 1.7ms preprocess, 48.5ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 55.9ms\n",
      "Speed: 1.6ms preprocess, 55.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.2ms\n",
      "Speed: 1.6ms preprocess, 50.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.2ms\n",
      "Speed: 1.5ms preprocess, 50.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 54.6ms\n",
      "Speed: 1.5ms preprocess, 54.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.5ms\n",
      "Speed: 1.8ms preprocess, 47.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 55.7ms\n",
      "Speed: 1.8ms preprocess, 55.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 65.7ms\n",
      "Speed: 1.3ms preprocess, 65.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 119.5ms\n",
      "Speed: 4.3ms preprocess, 119.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 60.1ms\n",
      "Speed: 1.6ms preprocess, 60.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 58.5ms\n",
      "Speed: 1.8ms preprocess, 58.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 53.7ms\n",
      "Speed: 1.4ms preprocess, 53.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 51.5ms\n",
      "Speed: 1.6ms preprocess, 51.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 53.3ms\n",
      "Speed: 1.5ms preprocess, 53.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 51.1ms\n",
      "Speed: 1.4ms preprocess, 51.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 52.7ms\n",
      "Speed: 1.6ms preprocess, 52.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 56.2ms\n",
      "Speed: 2.1ms preprocess, 56.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 60.4ms\n",
      "Speed: 1.7ms preprocess, 60.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.0ms\n",
      "Speed: 1.6ms preprocess, 46.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 50.0ms\n",
      "Speed: 1.6ms preprocess, 50.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.9ms\n",
      "Speed: 1.9ms preprocess, 47.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.6ms\n",
      "Speed: 1.9ms preprocess, 47.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 46.4ms\n",
      "Speed: 1.6ms preprocess, 46.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 42.8ms\n",
      "Speed: 1.5ms preprocess, 42.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 43.2ms\n",
      "Speed: 1.3ms preprocess, 43.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 47.9ms\n",
      "Speed: 1.5ms preprocess, 47.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 50.8ms\n",
      "Speed: 1.4ms preprocess, 50.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 51.3ms\n",
      "Speed: 1.5ms preprocess, 51.3ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 91.3ms\n",
      "Speed: 2.4ms preprocess, 91.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 60.8ms\n",
      "Speed: 2.0ms preprocess, 60.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 63.3ms\n",
      "Speed: 1.5ms preprocess, 63.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 43\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ret:\n\u001B[32m     41\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m results = \u001B[43mperson_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpersist\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m]\n\u001B[32m     45\u001B[39m annotated_frame = frame.copy()\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m box, score, \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(results.boxes.xyxy, results.boxes.conf, results.boxes.id):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/engine/model.py:583\u001B[39m, in \u001B[36mModel.track\u001B[39m\u001B[34m(self, source, stream, persist, **kwargs)\u001B[39m\n\u001B[32m    581\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m] = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m  \u001B[38;5;66;03m# batch-size 1 for tracking in videos\u001B[39;00m\n\u001B[32m    582\u001B[39m kwargs[\u001B[33m\"\u001B[39m\u001B[33mmode\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33m\"\u001B[39m\u001B[33mtrack\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m583\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/engine/model.py:540\u001B[39m, in \u001B[36mModel.predict\u001B[39m\u001B[34m(self, source, stream, predictor, **kwargs)\u001B[39m\n\u001B[32m    538\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.predictor, \u001B[33m\"\u001B[39m\u001B[33mset_prompts\u001B[39m\u001B[33m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[32m    539\u001B[39m     \u001B[38;5;28mself\u001B[39m.predictor.set_prompts(prompts)\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.predictor.predict_cli(source=source) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m=\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/engine/predictor.py:225\u001B[39m, in \u001B[36mBasePredictor.__call__\u001B[39m\u001B[34m(self, source, model, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs)\n\u001B[32m    224\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m225\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.stream_inference(source, model, *args, **kwargs))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/torch/utils/_contextlib.py:36\u001B[39m, in \u001B[36m_wrap_generator.<locals>.generator_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     34\u001B[39m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m         response = gen.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m     39\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     40\u001B[39m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/engine/predictor.py:335\u001B[39m, in \u001B[36mBasePredictor.stream_inference\u001B[39m\u001B[34m(self, source, model, *args, **kwargs)\u001B[39m\n\u001B[32m    333\u001B[39m \u001B[38;5;66;03m# Postprocess\u001B[39;00m\n\u001B[32m    334\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[32m2\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m335\u001B[39m     \u001B[38;5;28mself\u001B[39m.results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpostprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim0s\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[38;5;28mself\u001B[39m.run_callbacks(\u001B[33m\"\u001B[39m\u001B[33mon_predict_postprocess_end\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    338\u001B[39m \u001B[38;5;66;03m# Visualize, save, write results\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/models/yolo/detect/predict.py:54\u001B[39m, in \u001B[36mDetectionPredictor.postprocess\u001B[39m\u001B[34m(self, preds, img, orig_imgs, **kwargs)\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Post-process predictions and return a list of Results objects.\u001B[39;00m\n\u001B[32m     35\u001B[39m \n\u001B[32m     36\u001B[39m \u001B[33;03mThis method applies non-maximum suppression to raw model predictions and prepares them for visualization and\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     51\u001B[39m \u001B[33;03m    >>> processed_results = predictor.postprocess(preds, img, orig_imgs)\u001B[39;00m\n\u001B[32m     52\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     53\u001B[39m save_feats = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m_feats\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m54\u001B[39m preds = \u001B[43mnms\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnon_max_suppression\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     56\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     57\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43miou\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     59\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43magnostic_nms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     60\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_det\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmax_det\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     61\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnc\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdetect\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     62\u001B[39m \u001B[43m    \u001B[49m\u001B[43mend2end\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mend2end\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     63\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrotated\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43margs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mobb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_idxs\u001B[49m\u001B[43m=\u001B[49m\u001B[43msave_feats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(orig_imgs, \u001B[38;5;28mlist\u001B[39m):  \u001B[38;5;66;03m# input images are a torch.Tensor, not a list\u001B[39;00m\n\u001B[32m     68\u001B[39m     orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/utils/nms.py:86\u001B[39m, in \u001B[36mnon_max_suppression\u001B[39m\u001B[34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, rotated, end2end, return_idxs)\u001B[39m\n\u001B[32m     84\u001B[39m prediction = prediction.transpose(-\u001B[32m1\u001B[39m, -\u001B[32m2\u001B[39m)  \u001B[38;5;66;03m# shape(1,84,6300) to shape(1,6300,84)\u001B[39;00m\n\u001B[32m     85\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m rotated:\n\u001B[32m---> \u001B[39m\u001B[32m86\u001B[39m     prediction[..., :\u001B[32m4\u001B[39m] = \u001B[43mxywh2xyxy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m[\u001B[49m\u001B[43m.\u001B[49m\u001B[43m.\u001B[49m\u001B[43m.\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# xywh to xyxy\u001B[39;00m\n\u001B[32m     88\u001B[39m t = time.time()\n\u001B[32m     89\u001B[39m output = [torch.zeros((\u001B[32m0\u001B[39m, \u001B[32m6\u001B[39m + extra), device=prediction.device)] * bs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/utils/ops.py:279\u001B[39m, in \u001B[36mxywh2xyxy\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Convert bounding box coordinates from (x, y, width, height) format to (x1, y1, x2, y2) format where (x1, y1) is\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[33;03mthe top-left corner and (x2, y2) is the bottom-right corner. Note: ops per 2 channels faster than per channel.\u001B[39;00m\n\u001B[32m    271\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    276\u001B[39m \u001B[33;03m    (np.ndarray | torch.Tensor): Bounding box coordinates in (x1, y1, x2, y2) format.\u001B[39;00m\n\u001B[32m    277\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    278\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m x.shape[-\u001B[32m1\u001B[39m] == \u001B[32m4\u001B[39m, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33minput shape last dimension expected 4 but input shape is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m y = \u001B[43mempty_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# faster than clone/copy\u001B[39;00m\n\u001B[32m    280\u001B[39m xy = x[..., :\u001B[32m2\u001B[39m]  \u001B[38;5;66;03m# centers\u001B[39;00m\n\u001B[32m    281\u001B[39m wh = x[..., \u001B[32m2\u001B[39m:] / \u001B[32m2\u001B[39m  \u001B[38;5;66;03m# half width-height\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/ultralytics/utils/ops.py:696\u001B[39m, in \u001B[36mempty_like\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m    693\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mempty_like\u001B[39m(x):\n\u001B[32m    694\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create empty torch.Tensor or np.ndarray with same shape as input and float32 dtype.\"\"\"\u001B[39;00m\n\u001B[32m    695\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m696\u001B[39m         \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mempty_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, torch.Tensor) \u001B[38;5;28;01melse\u001B[39;00m np.empty_like(x, dtype=np.float32)\n\u001B[32m    697\u001B[39m     )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/async_helpers.py:128\u001B[39m, in \u001B[36m_pseudo_sync_runner\u001B[39m\u001B[34m(coro)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    121\u001B[39m \u001B[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001B[39;00m\n\u001B[32m    122\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    125\u001B[39m \u001B[33;03mCredit to Nathaniel Smith\u001B[39;00m\n\u001B[32m    126\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     coro.send(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exc.value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3413\u001B[39m, in \u001B[36mInteractiveShell.run_cell_async\u001B[39m\u001B[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001B[39m\n\u001B[32m   3409\u001B[39m exec_count = \u001B[38;5;28mself\u001B[39m.execution_count\n\u001B[32m   3410\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result.error_in_exec:\n\u001B[32m   3411\u001B[39m     \u001B[38;5;66;03m# Store formatted traceback and error details\u001B[39;00m\n\u001B[32m   3412\u001B[39m     \u001B[38;5;28mself\u001B[39m.history_manager.exceptions[exec_count] = (\n\u001B[32m-> \u001B[39m\u001B[32m3413\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_format_exception_for_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror_in_exec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3414\u001B[39m     )\n\u001B[32m   3416\u001B[39m \u001B[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001B[39;00m\n\u001B[32m   3417\u001B[39m \u001B[38;5;28mself\u001B[39m.execution_count += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3467\u001B[39m, in \u001B[36mInteractiveShell._format_exception_for_storage\u001B[39m\u001B[34m(self, exception, filename, running_compiled_code)\u001B[39m\n\u001B[32m   3464\u001B[39m         stb = evalue._render_traceback_()\n\u001B[32m   3465\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3466\u001B[39m         \u001B[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3467\u001B[39m         stb = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mInteractiveTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3468\u001B[39m \u001B[43m            \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m   3469\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3470\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   3471\u001B[39m     \u001B[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001B[39;00m\n\u001B[32m   3472\u001B[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/ultratb.py:1179\u001B[39m, in \u001B[36mAutoFormattedTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m   1177\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1178\u001B[39m     \u001B[38;5;28mself\u001B[39m.tb = etb\n\u001B[32m-> \u001B[39m\u001B[32m1179\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFormattedTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1180\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[32m   1181\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/ultratb.py:1050\u001B[39m, in \u001B[36mFormattedTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m   1047\u001B[39m mode = \u001B[38;5;28mself\u001B[39m.mode\n\u001B[32m   1048\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose_modes:\n\u001B[32m   1049\u001B[39m     \u001B[38;5;66;03m# Verbose modes need a full traceback\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1050\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVerboseTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1051\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[32m   1052\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1053\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mDocs\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1054\u001B[39m     \u001B[38;5;66;03m# return DocTB\u001B[39;00m\n\u001B[32m   1055\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m DocTB(\n\u001B[32m   1056\u001B[39m         theme_name=\u001B[38;5;28mself\u001B[39m._theme_name,\n\u001B[32m   1057\u001B[39m         call_pdb=\u001B[38;5;28mself\u001B[39m.call_pdb,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1065\u001B[39m         etype, evalue, etb, tb_offset, \u001B[32m1\u001B[39m\n\u001B[32m   1066\u001B[39m     )  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/ultratb.py:858\u001B[39m, in \u001B[36mVerboseTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m    849\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstructured_traceback\u001B[39m(\n\u001B[32m    850\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    851\u001B[39m     etype: \u001B[38;5;28mtype\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    855\u001B[39m     context: \u001B[38;5;28mint\u001B[39m = \u001B[32m5\u001B[39m,\n\u001B[32m    856\u001B[39m ) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    857\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m858\u001B[39m     formatted_exceptions: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_exception_as_a_whole\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    859\u001B[39m \u001B[43m        \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\n\u001B[32m    860\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    862\u001B[39m     termsize = \u001B[38;5;28mmin\u001B[39m(\u001B[32m75\u001B[39m, get_terminal_size()[\u001B[32m0\u001B[39m])\n\u001B[32m    863\u001B[39m     theme = theme_table[\u001B[38;5;28mself\u001B[39m._theme_name]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/ultratb.py:770\u001B[39m, in \u001B[36mVerboseTB.format_exception_as_a_whole\u001B[39m\u001B[34m(self, etype, evalue, etb, context, tb_offset)\u001B[39m\n\u001B[32m    760\u001B[39m         frames.append(\n\u001B[32m    761\u001B[39m             theme_table[\u001B[38;5;28mself\u001B[39m._theme_name].format(\n\u001B[32m    762\u001B[39m                 [\n\u001B[32m   (...)\u001B[39m\u001B[32m    767\u001B[39m             )\n\u001B[32m    768\u001B[39m         )\n\u001B[32m    769\u001B[39m         skipped = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m770\u001B[39m     frames.append(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_record\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    771\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m skipped:\n\u001B[32m    772\u001B[39m     frames.append(\n\u001B[32m    773\u001B[39m         theme_table[\u001B[38;5;28mself\u001B[39m._theme_name].format(\n\u001B[32m    774\u001B[39m             [\n\u001B[32m   (...)\u001B[39m\u001B[32m    779\u001B[39m         )\n\u001B[32m    780\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/ultratb.py:648\u001B[39m, in \u001B[36mVerboseTB.format_record\u001B[39m\u001B[34m(self, frame_info)\u001B[39m\n\u001B[32m    645\u001B[39m result += \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m call \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    646\u001B[39m result += \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcall\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    647\u001B[39m result += theme_table[\u001B[38;5;28mself\u001B[39m._theme_name].format(\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m     \u001B[43m_format_traceback_lines\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m        \u001B[49m\u001B[43mframe_info\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlines\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtheme_table\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_theme_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    651\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhas_colors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    652\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlvals_toks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    653\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    654\u001B[39m )\n\u001B[32m    655\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/IPython/core/tbtools.py:99\u001B[39m, in \u001B[36m_format_traceback_lines\u001B[39m\u001B[34m(lines, theme, has_colors, lvals_toks)\u001B[39m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     98\u001B[39m lineno = stack_line.lineno\n\u001B[32m---> \u001B[39m\u001B[32m99\u001B[39m line = \u001B[43mstack_line\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpygmented\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_colors\u001B[49m\u001B[43m)\u001B[49m.rstrip(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m) + \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stack_line.is_current:\n\u001B[32m    101\u001B[39m     \u001B[38;5;66;03m# This is the line with the error\u001B[39;00m\n\u001B[32m    102\u001B[39m     pad = numbers_width - \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mstr\u001B[39m(lineno))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/stack_data/core.py:391\u001B[39m, in \u001B[36mLine.render\u001B[39m\u001B[34m(self, markers, strip_leading_indent, pygmented, escape_html)\u001B[39m\n\u001B[32m    389\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m pygmented \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.frame_info.scope:\n\u001B[32m    390\u001B[39m     assert_(\u001B[38;5;129;01mnot\u001B[39;00m markers, \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mCannot use pygmented with markers\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m--> \u001B[39m\u001B[32m391\u001B[39m     start_line, lines = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mframe_info\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_pygmented_scope_lines\u001B[49m\n\u001B[32m    392\u001B[39m     result = lines[\u001B[38;5;28mself\u001B[39m.lineno - start_line]\n\u001B[32m    393\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m strip_leading_indent:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/stack_data/utils.py:145\u001B[39m, in \u001B[36mcached_property.cached_property_wrapper\u001B[39m\u001B[34m(self, obj, _cls)\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    143\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m value = obj.\u001B[34m__dict__\u001B[39m[\u001B[38;5;28mself\u001B[39m.func.\u001B[34m__name__\u001B[39m] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/stack_data/core.py:824\u001B[39m, in \u001B[36mFrameInfo._pygmented_scope_lines\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    821\u001B[39m     ranges = []\n\u001B[32m    823\u001B[39m code = atext.get_text(scope)\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m lines = \u001B[43m_pygmented_with_ranges\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mranges\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    826\u001B[39m start_line = \u001B[38;5;28mself\u001B[39m.source.line_range(scope)[\u001B[32m0\u001B[39m]\n\u001B[32m    828\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m start_line, lines\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/stack_data/utils.py:166\u001B[39m, in \u001B[36m_pygmented_with_ranges\u001B[39m\u001B[34m(formatter, code, ranges)\u001B[39m\n\u001B[32m    164\u001B[39m lexer = MyLexer(stripnl=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    165\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m166\u001B[39m     highlighted = \u001B[43mpygments\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhighlight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    167\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    168\u001B[39m     \u001B[38;5;66;03m# When pygments fails, prefer code without highlighting over crashing\u001B[39;00m\n\u001B[32m    169\u001B[39m     highlighted = code\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/__init__.py:82\u001B[39m, in \u001B[36mhighlight\u001B[39m\u001B[34m(code, lexer, formatter, outfile)\u001B[39m\n\u001B[32m     77\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhighlight\u001B[39m(code, lexer, formatter, outfile=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m     78\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     79\u001B[39m \u001B[33;03m    This is the most high-level highlighting function. It combines `lex` and\u001B[39;00m\n\u001B[32m     80\u001B[39m \u001B[33;03m    `format` in one function.\u001B[39;00m\n\u001B[32m     81\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mlex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlexer\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/__init__.py:64\u001B[39m, in \u001B[36mformat\u001B[39m\u001B[34m(tokens, formatter, outfile)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m outfile:\n\u001B[32m     63\u001B[39m     realoutfile = \u001B[38;5;28mgetattr\u001B[39m(formatter, \u001B[33m'\u001B[39m\u001B[33mencoding\u001B[39m\u001B[33m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m BytesIO() \u001B[38;5;129;01mor\u001B[39;00m StringIO()\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     \u001B[43mformatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrealoutfile\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m realoutfile.getvalue()\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/formatters/terminal256.py:250\u001B[39m, in \u001B[36mTerminal256Formatter.format\u001B[39m\u001B[34m(self, tokensource, outfile)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat\u001B[39m(\u001B[38;5;28mself\u001B[39m, tokensource, outfile):\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFormatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokensource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/formatter.py:124\u001B[39m, in \u001B[36mFormatter.format\u001B[39m\u001B[34m(self, tokensource, outfile)\u001B[39m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.encoding:\n\u001B[32m    122\u001B[39m     \u001B[38;5;66;03m# wrap the outfile in a StreamWriter\u001B[39;00m\n\u001B[32m    123\u001B[39m     outfile = codecs.lookup(\u001B[38;5;28mself\u001B[39m.encoding)[\u001B[32m3\u001B[39m](outfile)\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_unencoded\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokensource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutfile\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/formatters/terminal256.py:256\u001B[39m, in \u001B[36mTerminal256Formatter.format_unencoded\u001B[39m\u001B[34m(self, tokensource, outfile)\u001B[39m\n\u001B[32m    253\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.linenos:\n\u001B[32m    254\u001B[39m     \u001B[38;5;28mself\u001B[39m._write_lineno(outfile)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mttype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtokensource\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnot_found\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m    258\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwhile\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mttype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnot_found\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/stack_data/utils.py:158\u001B[39m, in \u001B[36m_pygmented_with_ranges.<locals>.MyLexer.get_tokens\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    156\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, text):\n\u001B[32m    157\u001B[39m     length = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mttype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    159\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43many\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m \u001B[49m\u001B[43m<\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength\u001B[49m\u001B[43m \u001B[49m\u001B[43m<\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mranges\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m            \u001B[49m\u001B[43mttype\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mttype\u001B[49m\u001B[43m.\u001B[49m\u001B[43mExecutingNode\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/lexer.py:270\u001B[39m, in \u001B[36mLexer.get_tokens.<locals>.streamer\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    269\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstreamer\u001B[39m():\n\u001B[32m--> \u001B[39m\u001B[32m270\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_tokens_unprocessed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/miniconda3/envs/torch_ml/lib/python3.11/site-packages/pygments/lexer.py:712\u001B[39m, in \u001B[36mRegexLexer.get_tokens_unprocessed\u001B[39m\u001B[34m(self, text, stack)\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[32m1\u001B[39m:\n\u001B[32m    711\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m rexmatch, action, new_state \u001B[38;5;129;01min\u001B[39;00m statetokens:\n\u001B[32m--> \u001B[39m\u001B[32m712\u001B[39m         m = \u001B[43mrexmatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m m:\n\u001B[32m    714\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T12:53:28.465485Z",
     "start_time": "2025-11-26T12:52:47.813236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "model = YOLO(\"yolov8s-pose.pt\")\n",
    "\n",
    "history = defaultdict(lambda: [])\n",
    "max_hist = 60  # 2 секунды при 30 fps\n",
    "\n",
    "cap = cv2.VideoCapture(\"vid_2.mp4\")\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    annotated = frame.copy()\n",
    "\n",
    "    results = model.track(\n",
    "        frame,\n",
    "        persist=True,\n",
    "        tracker=\"bytetrack.yaml\",\n",
    "        classes=[0],\n",
    "        conf=0.3,\n",
    "        iou=0.5,\n",
    "        verbose=False\n",
    "    )[0]\n",
    "\n",
    "    if results.boxes.id is not None:\n",
    "        for box, track_id, kpts_xy, kpts_conf in zip(\n",
    "            results.boxes.xyxy,\n",
    "            results.boxes.id,\n",
    "            results.keypoints.xy.cpu().numpy(),\n",
    "            results.keypoints.conf.cpu().numpy()\n",
    "        ):\n",
    "            track_id = int(track_id)\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            lk = kpts_xy[13]\n",
    "            rk = kpts_xy[14]\n",
    "            conf_l, conf_r = kpts_conf[13], kpts_conf[14]\n",
    "\n",
    "            if conf_l > 0.5 and conf_r > 0.5 and lk[0] > 0 and rk[0] > 0:\n",
    "                lk_pt = (int(lk[0]), int(lk[1]))\n",
    "                rk_pt = (int(rk[0]), int(rk[1]))\n",
    "\n",
    "                cv2.circle(annotated, lk_pt, 10, (255, 0, 255), -1)\n",
    "                cv2.circle(annotated, rk_pt, 10, (255, 0, 255), -1)\n",
    "\n",
    "                avg_y = (lk[1] + rk[1]) / 2\n",
    "                history[track_id].append(avg_y)\n",
    "                if len(history[track_id]) > max_hist:\n",
    "                    history[track_id].pop(0)\n",
    "\n",
    "                if len(history[track_id]) > 15:\n",
    "                    y_vals = np.array(history[track_id])\n",
    "                    peaks, _ = find_peaks(y_vals, distance=int(fps*0.4), prominence=10)\n",
    "                    steps_per_min = len(peaks) / (len(y_vals)/fps) * 60\n",
    "                    step_text = f\"{int(steps_per_min)} steps/min\"\n",
    "                else:\n",
    "                    step_text = \"—\"\n",
    "\n",
    "                cv2.putText(annotated, step_text, (x1, y1-10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(annotated, f\"ID:{track_id}\", (x1, y1-30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"YOLOv8-pose + ByteTrack\", annotated)\n",
    "    if cv2.waitKey(1) == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9002e08ccc1bb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d0f4f0a2279e1488"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
