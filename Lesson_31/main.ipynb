{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-22T13:12:18.195946Z",
     "start_time": "2025-12-22T13:12:16.547198Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('combined_data.csv', encoding='latin-1')\n",
    "\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label                                               text\n",
       "0      1  ounce feather bowl hummingbird opec moment ala...\n",
       "1      1  wulvob get your medircations online qnb ikud v...\n",
       "2      0   computer connection from cnn com wednesday es...\n",
       "3      1  university degree obtain a prosperous future m...\n",
       "4      0  thanks for all your answers guys i know i shou..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>computer connection from cnn com wednesday es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:12:21.599385Z",
     "start_time": "2025-12-22T13:12:18.355831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-eng')\n",
    "\n",
    "df = pd.read_csv('combined_data.csv', encoding='latin-1')\n",
    "\n",
    "df = df.drop_duplicates(subset='text')\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "def clean_basic(text):\n",
    "    \"\"\"Только нижний регистр и удаление лишних пробелов\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_no_stopwords(text):\n",
    "    \"\"\"Без стоп-слов, но с пунктуацией\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_full(text):\n",
    "    \"\"\"Полная очистка: без пунктуации, чисел, стоп-слов\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_with_lemmatization(text):\n",
    "    \"\"\"Полная очистка + лемматизация\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_with_stemming(text):\n",
    "    \"\"\"Полная очистка + стемминг\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_keep_special_chars(text):\n",
    "    \"\"\"Сохраняем специальные символы ($, !, ?), которые могут быть важны для спама\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\$\\!\\?\\*\\%\\.]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "id": "5e8972e1de9524eb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/artemsotnikov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/artemsotnikov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Error loading omw-eng: Package 'omw-eng' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:15:07.475066Z",
     "start_time": "2025-12-22T13:12:22.039974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['text_basic'] = df['text'].apply(clean_basic)\n",
    "df['text_no_stopwords'] = df['text'].apply(clean_no_stopwords)\n",
    "df['text_full'] = df['text'].apply(clean_full)\n",
    "df['text_lemmatized'] = df['text'].apply(clean_with_lemmatization)\n",
    "df['text_stemmed'] = df['text'].apply(clean_with_stemming)\n",
    "df['text_special_chars'] = df['text'].apply(clean_keep_special_chars)\n",
    "\n",
    "sample_idx = 10\n",
    "print(\"Оригинал:\")\n",
    "print(df['text'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Только нижний регистр:\")\n",
    "print(df['text_basic'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"Без стоп-слов:\")\n",
    "print(df['text_no_stopwords'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"Полная очистка:\")\n",
    "print(df['text_full'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"С лемматизацией:\")\n",
    "print(df['text_lemmatized'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"Со стеммингом:\")\n",
    "print(df['text_stemmed'].iloc[sample_idx])\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "print(\"Со спецсимволами:\")\n",
    "print(df['text_special_chars'].iloc[sample_idx])"
   ],
   "id": "66a01bd860d89dc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинал:\n",
      "my dear fellow do you feel insecure about your penis size you need our new improved megadik penis enlargement pills please read on did you know megadik was featured in leading mens magazines such as fhm maxim plus many others and rated no escapenumber choice for penis enlargement Â» gain escapenumber inches in length Â» increase your penis width girth by upto escapenumber Â» produce stronger rock hard erections Â» escapenumber safe to take with no side effects Â» doctor approved and recommended Â» fast shipping worldwide you have nothing to lose just a lot to gain http slasy net regards escapelong remains in escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple in this wescapenumberrld i am escapelong man and alescapenumberng with all thescapenumberse milliescapenumberns although far from perfect especially in that it precludes a vast waldron\n",
      "\n",
      "==================================================\n",
      "\n",
      "Только нижний регистр:\n",
      "my dear fellow do you feel insecure about your penis size you need our new improved megadik penis enlargement pills please read on did you know megadik was featured in leading mens magazines such as fhm maxim plus many others and rated no escapenumber choice for penis enlargement â» gain escapenumber inches in length â» increase your penis width girth by upto escapenumber â» produce stronger rock hard erections â» escapenumber safe to take with no side effects â» doctor approved and recommended â» fast shipping worldwide you have nothing to lose just a lot to gain http slasy net regards escapelong remains in escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple in this wescapenumberrld i am escapelong man and alescapenumberng with all thescapenumberse milliescapenumberns although far from perfect especially in that it precludes a vast waldron\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Без стоп-слов:\n",
      "dear fellow feel insecure penis size need new improved megadik penis enlargement pills please read know megadik featured leading mens magazines fhm maxim plus many others rated escapenumber choice penis enlargement â» gain escapenumber inches length â» increase penis width girth upto escapenumber â» produce stronger rock hard erections â» escapenumber safe take side effects â» doctor approved recommended â» fast shipping worldwide nothing lose lot gain http slasy net regards escapelong remains escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple wescapenumberrld escapelong man alescapenumberng thescapenumberse milliescapenumberns although far perfect especially precludes vast waldron\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Полная очистка:\n",
      "dear fellow feel insecure penis size need new improved megadik penis enlargement pills please read know megadik featured leading mens magazines fhm maxim plus many others rated escapenumber choice penis enlargement gain escapenumber inches length increase penis width girth upto escapenumber produce stronger rock hard erections escapenumber safe take side effects doctor approved recommended fast shipping worldwide nothing lose lot gain http slasy net regards escapelong remains escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple wescapenumberrld escapelong man alescapenumberng thescapenumberse milliescapenumberns although far perfect especially precludes vast waldron\n",
      "\n",
      "------------------------------\n",
      "\n",
      "С лемматизацией:\n",
      "dear fellow feel insecure penis size need new improved megadik penis enlargement pill please read know megadik featured leading men magazine fhm maxim plus many others rated escapenumber choice penis enlargement gain escapenumber inch length increase penis width girth upto escapenumber produce stronger rock hard erection escapenumber safe take side effect doctor approved recommended fast shipping worldwide nothing lose lot gain http slasy net regard escapelong remains escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple wescapenumberrld escapelong man alescapenumberng thescapenumberse milliescapenumberns although far perfect especially precludes vast waldron\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Со стеммингом:\n",
      "dear fellow feel insecur peni size need new improv megadik peni enlarg pill pleas read know megadik featur lead men magazin fhm maxim plu mani other rate escapenumb choic peni enlarg gain escapenumb inch length increas peni width girth upto escapenumb produc stronger rock hard erect escapenumb safe take side effect doctor approv recommend fast ship worldwid noth lose lot gain http slasi net regard escapelong remain escapelong use escapenumberf milliescapenumbern escapenumberf peescapenumberpl wescapenumberrld escapelong man alescapenumberng thescapenumbers milliescapenumbern although far perfect especi preclud vast waldron\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Со спецсимволами:\n",
      "my dear fellow do you feel insecure about your penis size you need our new improved megadik penis enlargement pills please read on did you know megadik was featured in leading mens magazines such as fhm maxim plus many others and rated no escapenumber choice for penis enlargement gain escapenumber inches in length increase your penis width girth by upto escapenumber produce stronger rock hard erections escapenumber safe to take with no side effects doctor approved and recommended fast shipping worldwide you have nothing to lose just a lot to gain http slasy net regards escapelong remains in escapelong use escapenumberf milliescapenumberns escapenumberf peescapenumberple in this wescapenumberrld i am escapelong man and alescapenumberng with all thescapenumberse milliescapenumberns although far from perfect especially in that it precludes a vast waldron\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:15:07.749809Z",
     "start_time": "2025-12-22T13:15:07.740897Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "cbf8092d6aec343c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  ounce feather bowl hummingbird opec moment ala...   \n",
       "1      1  wulvob get your medircations online qnb ikud v...   \n",
       "2      0   computer connection from cnn com wednesday es...   \n",
       "3      1  university degree obtain a prosperous future m...   \n",
       "4      0  thanks for all your answers guys i know i shou...   \n",
       "\n",
       "                                          text_basic  \\\n",
       "0  ounce feather bowl hummingbird opec moment ala...   \n",
       "1  wulvob get your medircations online qnb ikud v...   \n",
       "2  computer connection from cnn com wednesday esc...   \n",
       "3  university degree obtain a prosperous future m...   \n",
       "4  thanks for all your answers guys i know i shou...   \n",
       "\n",
       "                                   text_no_stopwords  \\\n",
       "0  ounce feather bowl hummingbird opec moment ala...   \n",
       "1  wulvob get medircations online qnb ikud viagra...   \n",
       "2  computer connection cnn com wednesday escapenu...   \n",
       "3  university degree obtain prosperous future mon...   \n",
       "4  thanks answers guys know checked rsync manual ...   \n",
       "\n",
       "                                           text_full  \\\n",
       "0  ounce feather bowl hummingbird opec moment ala...   \n",
       "1  wulvob get medircations online qnb ikud viagra...   \n",
       "2  computer connection cnn com wednesday escapenu...   \n",
       "3  university degree obtain prosperous future mon...   \n",
       "4  thanks answers guys know checked rsync manual ...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  ounce feather bowl hummingbird opec moment ala...   \n",
       "1  wulvob get medircations online qnb ikud viagra...   \n",
       "2  computer connection cnn com wednesday escapenu...   \n",
       "3  university degree obtain prosperous future mon...   \n",
       "4  thanks answer guy know checked rsync manual wo...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  ounc feather bowl hummingbird opec moment alab...   \n",
       "1  wulvob get medirc onlin qnb ikud viagra escape...   \n",
       "2  comput connect cnn com wednesday escapenumb ma...   \n",
       "3  univers degre obtain prosper futur money earn ...   \n",
       "4  thank answer guy know check rsync manual would...   \n",
       "\n",
       "                                  text_special_chars  \n",
       "0  ounce feather bowl hummingbird opec moment ala...  \n",
       "1  wulvob get your medircations online qnb ikud v...  \n",
       "2  computer connection from cnn com wednesday esc...  \n",
       "3  university degree obtain a prosperous future m...  \n",
       "4  thanks for all your answers guys i know i shou...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_basic</th>\n",
       "      <th>text_no_stopwords</th>\n",
       "      <th>text_full</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_special_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "      <td>ounc feather bowl hummingbird opec moment alab...</td>\n",
       "      <td>ounce feather bowl hummingbird opec moment ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "      <td>wulvob get medircations online qnb ikud viagra...</td>\n",
       "      <td>wulvob get medircations online qnb ikud viagra...</td>\n",
       "      <td>wulvob get medircations online qnb ikud viagra...</td>\n",
       "      <td>wulvob get medirc onlin qnb ikud viagra escape...</td>\n",
       "      <td>wulvob get your medircations online qnb ikud v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>computer connection from cnn com wednesday es...</td>\n",
       "      <td>computer connection from cnn com wednesday esc...</td>\n",
       "      <td>computer connection cnn com wednesday escapenu...</td>\n",
       "      <td>computer connection cnn com wednesday escapenu...</td>\n",
       "      <td>computer connection cnn com wednesday escapenu...</td>\n",
       "      <td>comput connect cnn com wednesday escapenumb ma...</td>\n",
       "      <td>computer connection from cnn com wednesday esc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "      <td>university degree obtain prosperous future mon...</td>\n",
       "      <td>university degree obtain prosperous future mon...</td>\n",
       "      <td>university degree obtain prosperous future mon...</td>\n",
       "      <td>univers degre obtain prosper futur money earn ...</td>\n",
       "      <td>university degree obtain a prosperous future m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "      <td>thanks answers guys know checked rsync manual ...</td>\n",
       "      <td>thanks answers guys know checked rsync manual ...</td>\n",
       "      <td>thanks answer guy know checked rsync manual wo...</td>\n",
       "      <td>thank answer guy know check rsync manual would...</td>\n",
       "      <td>thanks for all your answers guys i know i shou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:15:11.048041Z",
     "start_time": "2025-12-22T13:15:07.816420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "preprocess_cols = [\n",
    "    'text_basic',\n",
    "    'text_no_stopwords',\n",
    "    'text_full',\n",
    "    'text_lemmatized',\n",
    "    'text_stemmed',\n",
    "    'text_special_chars'\n",
    "]\n",
    "\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'SVM (linear)': LinearSVC(max_iter=2000, dual=False, random_state=42, C=1.0),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "embeddings = ['Word2Vec', 'spaCy', 'TF-IDF']\n",
    "\n",
    "all_results = {}"
   ],
   "id": "dee17d288b620bc5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/artemsotnikov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/artemsotnikov/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T14:17:37.507405Z",
     "start_time": "2025-12-22T14:17:37.237825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def run_experiment(col_name, df):\n",
    "    print(f\"Эксперимент для предобработки: {col_name.upper()}\")\n",
    "\n",
    "    print(\"Обучение Word2Vec...\")\n",
    "    tokens = df[col_name].apply(word_tokenize).tolist()\n",
    "    w2v_model = Word2Vec(sentences=tokens, vector_size=150, window=5, min_count=2, workers=4, epochs=10)\n",
    "\n",
    "    def avg_w2v(tokens_list):\n",
    "        vecs = [w2v_model.wv[t] for t in tokens_list if t in w2v_model.wv]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(150)\n",
    "\n",
    "    X_w2v = np.array([avg_w2v(t) for t in tokens])\n",
    "\n",
    "    print(\"  spaCy векторы...\")\n",
    "    def safe_spacy_vector(text):\n",
    "        text = str(text).strip()\n",
    "        if not text:\n",
    "            return np.zeros(96)\n",
    "        doc = nlp(text)\n",
    "        return doc.vector if len(doc.vector) > 0 else np.zeros(96)\n",
    "\n",
    "    X_spacy = np.array([safe_spacy_vector(text) for text in df[col_name]])\n",
    "\n",
    "    print(\"  TF-IDF (sparse)...\")\n",
    "    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "    X_tfidf_sparse = tfidf.fit_transform(df[col_name])\n",
    "\n",
    "    y = df['label']\n",
    "    results = {}\n",
    "\n",
    "    for emb_name, X in [('Word2Vec', X_w2v), ('spaCy', X_spacy)]:\n",
    "        print(f\"  → {emb_name}\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        results[emb_name] = {}\n",
    "\n",
    "        model_list = [\n",
    "            ('LogisticRegression', LogisticRegression(max_iter=1000)),\n",
    "            ('SVM (linear)', LinearSVC(max_iter=2000, dual=False, random_state=42)),\n",
    "            ('RandomForest', RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42))\n",
    "        ]\n",
    "\n",
    "        for name, model in model_list:\n",
    "            model.fit(X_train, y_train)\n",
    "            pred = model.predict(X_test)\n",
    "            acc = accuracy_score(y_test, pred)\n",
    "            results[emb_name][name] = acc\n",
    "            print(f\"     {name}: {acc:.4f}\")\n",
    "\n",
    "    print(f\"  → TF-IDF (sparse)\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf_sparse, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    results['TF-IDF'] = {}\n",
    "\n",
    "    tfidf_models = [\n",
    "        ('LogisticRegression', LogisticRegression(max_iter=1000, solver='saga')),\n",
    "        ('SVM (linear)', LinearSVC(max_iter=2000, dual=False, random_state=42))\n",
    "    ]\n",
    "\n",
    "    for name, model in tfidf_models:\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, pred)\n",
    "        results['TF-IDF'][name] = acc\n",
    "        print(f\"     {name}: {acc:.4f}\")\n",
    "\n",
    "    results['TF-IDF']['RandomForest'] = None\n",
    "\n",
    "    return results"
   ],
   "id": "cf97b39ab9c16ea0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T15:45:44.111300Z",
     "start_time": "2025-12-22T14:17:39.593309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for col in preprocess_cols:\n",
    "    all_results[col] = run_experiment(col, df)"
   ],
   "id": "c4e25a7a12d70f08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_BASIC\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9714\n",
      "     SVM (linear): 0.9703\n",
      "     RandomForest: 0.9840\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.8537\n",
      "     SVM (linear): 0.8537\n",
      "     RandomForest: 0.9380\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9853\n",
      "     SVM (linear): 0.9901\n",
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_NO_STOPWORDS\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9729\n",
      "     SVM (linear): 0.9731\n",
      "     RandomForest: 0.9860\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.8120\n",
      "     SVM (linear): 0.8116\n",
      "     RandomForest: 0.9179\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9861\n",
      "     SVM (linear): 0.9884\n",
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_FULL\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9725\n",
      "     SVM (linear): 0.9723\n",
      "     RandomForest: 0.9838\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.8104\n",
      "     SVM (linear): 0.8091\n",
      "     RandomForest: 0.8934\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9844\n",
      "     SVM (linear): 0.9876\n",
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_LEMMATIZED\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9721\n",
      "     SVM (linear): 0.9711\n",
      "     RandomForest: 0.9840\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.8090\n",
      "     SVM (linear): 0.8081\n",
      "     RandomForest: 0.8939\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9839\n",
      "     SVM (linear): 0.9875\n",
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_STEMMED\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9711\n",
      "     SVM (linear): 0.9702\n",
      "     RandomForest: 0.9826\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.7974\n",
      "     SVM (linear): 0.7956\n",
      "     RandomForest: 0.8809\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9843\n",
      "     SVM (linear): 0.9882\n",
      "\n",
      "============================================================\n",
      "Эксперимент для предобработки: TEXT_SPECIAL_CHARS\n",
      "============================================================\n",
      "\n",
      "Обучение Word2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  spaCy векторы...\n",
      "  TF-IDF (sparse)...\n",
      "  → Word2Vec\n",
      "     LogisticRegression: 0.9697\n",
      "     SVM (linear): 0.9695\n",
      "     RandomForest: 0.9851\n",
      "  → spaCy\n",
      "     LogisticRegression: 0.8585\n",
      "     SVM (linear): 0.8575\n",
      "     RandomForest: 0.9343\n",
      "  → TF-IDF (sparse)\n",
      "     LogisticRegression: 0.9839\n",
      "     SVM (linear): 0.9898\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T15:52:11.408942Z",
     "start_time": "2025-12-22T15:52:11.334650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{'Предобработка':<22} {'Word2Vec (best)':<18} {'spaCy (best)':<16} {'TF-IDF (best)':<16}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for col in preprocess_cols:\n",
    "    row = f\"{col:<22}\"\n",
    "\n",
    "    # Word2Vec\n",
    "    w2v_accs = [v for v in all_results[col]['Word2Vec'].values() if v is not None]\n",
    "    best_w2v = max(w2v_accs) if w2v_accs else 0\n",
    "    row += f\"{best_w2v:.4f} ({list(all_results[col]['Word2Vec'].keys())[w2v_accs.index(best_w2v)]})\".ljust(18)\n",
    "\n",
    "    # spaCy\n",
    "    spacy_accs = [v for v in all_results[col]['spaCy'].values() if v is not None]\n",
    "    best_spacy = max(spacy_accs) if spacy_accs else 0\n",
    "    row += f\"{best_spacy:.4f} ({list(all_results[col]['spaCy'].keys())[spacy_accs.index(best_spacy)]})\".ljust(16)\n",
    "\n",
    "    # TF-IDF\n",
    "    tfidf_accs = [v for v in all_results[col]['TF-IDF'].values() if v is not None]\n",
    "    best_tfidf = max(tfidf_accs) if tfidf_accs else 0\n",
    "    best_tfidf_model = list(all_results[col]['TF-IDF'].keys())[tfidf_accs.index(best_tfidf)]\n",
    "    row += f\"{best_tfidf:.4f} ({best_tfidf_model})\".ljust(16)\n",
    "\n",
    "    print(row)\n",
    "\n",
    "print(\"ЛУЧШИЕ РЕЗУЛЬТАТЫ ПО КАЖДОЙ ПРЕДОБРАБОТКЕ И ГЛОБАЛЬНЫЙ ПОБЕДИТЕЛЬ\")\n",
    "\n",
    "best_global_acc = 0\n",
    "best_global_combo = None\n",
    "\n",
    "for col in preprocess_cols:\n",
    "    print(f\"\\n{col.upper()}:\")\n",
    "    for emb in ['Word2Vec', 'spaCy', 'TF-IDF']:\n",
    "        if emb in all_results[col]:\n",
    "            for model_name, acc in all_results[col][emb].items():\n",
    "                if acc is not None:\n",
    "                    print(f\"  • {emb} + {model_name}: {acc:.4f}\")\n",
    "                    if acc > best_global_acc:\n",
    "                        best_global_acc = acc\n",
    "                        best_global_combo = (col, emb, model_name)\n",
    "\n",
    "\n",
    "print(f\"   {best_global_combo[0]} + {best_global_combo[1]} + {best_global_combo[2]}\")\n",
    "print(f\"   Accuracy: {best_global_acc:.4f}\")"
   ],
   "id": "d29b8f99fad8d467",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка          Word2Vec (best)    spaCy (best)     TF-IDF (best)   \n",
      "------------------------------------------------------------------------\n",
      "text_basic            0.9840 (RandomForest)0.9380 (RandomForest)0.9901 (SVM (linear))\n",
      "text_no_stopwords     0.9860 (RandomForest)0.9179 (RandomForest)0.9884 (SVM (linear))\n",
      "text_full             0.9838 (RandomForest)0.8934 (RandomForest)0.9876 (SVM (linear))\n",
      "text_lemmatized       0.9840 (RandomForest)0.8939 (RandomForest)0.9875 (SVM (linear))\n",
      "text_stemmed          0.9826 (RandomForest)0.8809 (RandomForest)0.9882 (SVM (linear))\n",
      "text_special_chars    0.9851 (RandomForest)0.9343 (RandomForest)0.9898 (SVM (linear))\n",
      "\n",
      "====================================================================================================\n",
      "ЛУЧШИЕ РЕЗУЛЬТАТЫ ПО КАЖДОЙ ПРЕДОБРАБОТКЕ И ГЛОБАЛЬНЫЙ ПОБЕДИТЕЛЬ\n",
      "====================================================================================================\n",
      "\n",
      "TEXT_BASIC:\n",
      "  • Word2Vec + LogisticRegression: 0.9714\n",
      "  • Word2Vec + SVM (linear): 0.9703\n",
      "  • Word2Vec + RandomForest: 0.9840\n",
      "  • spaCy + LogisticRegression: 0.8537\n",
      "  • spaCy + SVM (linear): 0.8537\n",
      "  • spaCy + RandomForest: 0.9380\n",
      "  • TF-IDF + LogisticRegression: 0.9853\n",
      "  • TF-IDF + SVM (linear): 0.9901\n",
      "\n",
      "TEXT_NO_STOPWORDS:\n",
      "  • Word2Vec + LogisticRegression: 0.9729\n",
      "  • Word2Vec + SVM (linear): 0.9731\n",
      "  • Word2Vec + RandomForest: 0.9860\n",
      "  • spaCy + LogisticRegression: 0.8120\n",
      "  • spaCy + SVM (linear): 0.8116\n",
      "  • spaCy + RandomForest: 0.9179\n",
      "  • TF-IDF + LogisticRegression: 0.9861\n",
      "  • TF-IDF + SVM (linear): 0.9884\n",
      "\n",
      "TEXT_FULL:\n",
      "  • Word2Vec + LogisticRegression: 0.9725\n",
      "  • Word2Vec + SVM (linear): 0.9723\n",
      "  • Word2Vec + RandomForest: 0.9838\n",
      "  • spaCy + LogisticRegression: 0.8104\n",
      "  • spaCy + SVM (linear): 0.8091\n",
      "  • spaCy + RandomForest: 0.8934\n",
      "  • TF-IDF + LogisticRegression: 0.9844\n",
      "  • TF-IDF + SVM (linear): 0.9876\n",
      "\n",
      "TEXT_LEMMATIZED:\n",
      "  • Word2Vec + LogisticRegression: 0.9721\n",
      "  • Word2Vec + SVM (linear): 0.9711\n",
      "  • Word2Vec + RandomForest: 0.9840\n",
      "  • spaCy + LogisticRegression: 0.8090\n",
      "  • spaCy + SVM (linear): 0.8081\n",
      "  • spaCy + RandomForest: 0.8939\n",
      "  • TF-IDF + LogisticRegression: 0.9839\n",
      "  • TF-IDF + SVM (linear): 0.9875\n",
      "\n",
      "TEXT_STEMMED:\n",
      "  • Word2Vec + LogisticRegression: 0.9711\n",
      "  • Word2Vec + SVM (linear): 0.9702\n",
      "  • Word2Vec + RandomForest: 0.9826\n",
      "  • spaCy + LogisticRegression: 0.7974\n",
      "  • spaCy + SVM (linear): 0.7956\n",
      "  • spaCy + RandomForest: 0.8809\n",
      "  • TF-IDF + LogisticRegression: 0.9843\n",
      "  • TF-IDF + SVM (linear): 0.9882\n",
      "\n",
      "TEXT_SPECIAL_CHARS:\n",
      "  • Word2Vec + LogisticRegression: 0.9697\n",
      "  • Word2Vec + SVM (linear): 0.9695\n",
      "  • Word2Vec + RandomForest: 0.9851\n",
      "  • spaCy + LogisticRegression: 0.8585\n",
      "  • spaCy + SVM (linear): 0.8575\n",
      "  • spaCy + RandomForest: 0.9343\n",
      "  • TF-IDF + LogisticRegression: 0.9839\n",
      "  • TF-IDF + SVM (linear): 0.9898\n",
      "   text_basic + TF-IDF + SVM (linear)\n",
      "   Accuracy: 0.9901\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bd84c020fd1a2afb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
